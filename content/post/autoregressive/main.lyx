#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Section -> #
\end_layout

\begin_layout Plain Layout
Subsection -> ## and so on
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
- data processing inequality
\end_layout

\begin_layout Plain Layout
- specific example: Bayes
\end_layout

\begin_layout Plain Layout
- discovery
\end_layout

\begin_layout Plain Layout
conclude
\end_layout

\end_inset


\end_layout

\begin_layout Section
Do auto-regressive models bite their own tail? The value of Gedankenexperiments
\end_layout

\begin_layout Standard
Autoregressive models use their output to arrive at predictions.
 In machine learning, this amounts to training on the output, i.e.
 generated data.
 Their existence justifies this apparent conundrum, there seems to be a
 benefit.
 
\end_layout

\begin_layout Standard
Or is there? The data processing inequality asserts that no pipeline, however
 deep, can arrive at new information.
 How can this be reconciled?
\end_layout

\begin_layout Subsection
The autoregressor
\end_layout

\begin_layout Standard
Consider a stream of data samples, 
\begin_inset Formula $\bm{x}_{\leq t}=\left(x_{1},\ldots,x_{t}\right)$
\end_inset

, for example elements of a time series, or images of cats.
 The autoregressor then is a model 
\begin_inset Formula $p$
\end_inset

 that generates the next output, 
\begin_inset Formula 
\[
\hat{x}_{t+1}\sim p\left(\bm{x}_{\leq t}\right),
\]

\end_inset


\end_layout

\begin_layout Standard
where we denote model-generated outputs with a hat 
\begin_inset Formula $\hat{x}$
\end_inset

.
 The model now becomes autoregressive if part of its input is its output.
 
\end_layout

\begin_layout Subsection
The data-processing inequality
\end_layout

\begin_layout Standard
The data-processing inequality posits that for any processing pipeline 
\begin_inset Formula $Y=f(X)$
\end_inset

, the pipeline cannot increase the information that the input contains about
 the source, 
\begin_inset Formula 
\[
\mathbb{I}\left[Z=f(Y);X\right]\leq\mathbb{I}\left[Y;X\right]\quad\forall\,f.
\]

\end_inset


\end_layout

\begin_layout Standard
Now, the autoregressor suggests that such a pipeline does exist: Namely,
 let's consider a ground truth distribution that we want to model, 
\begin_inset Formula $p^{*}$
\end_inset

.
 Ideally, the model should be as close as possible to 
\begin_inset Formula $p^{*}$
\end_inset

, that is the loss 
\begin_inset Formula $\mathbb{I}\left[\hat{p};p^{*}\right]$
\end_inset

 should be minimized.
 However, there is only a finite amount of samples from 
\begin_inset Formula $p^{*}$
\end_inset

 available to fit 
\begin_inset Formula $\hat{p}.$
\end_inset

 Can we hence use autoregressive feedback to define a sequence of predictors
\begin_inset Formula 
\[
X\rightarrow\hat{p}^{(1)}\sim X^{(1)}\rightarrow\hat{p}^{(2)}\sim X^{(2)}\rightarrow\ldots\rightarrow\hat{p}^{(n)}\sim X^{(n)}
\]

\end_inset


\end_layout

\begin_layout Standard
such that 
\begin_inset Formula $\mathbb{I}\left[X;X^{(n)}\right]\geq\mathbb{I}\left[X;X^{(1)}\right]$
\end_inset

?
\end_layout

\begin_layout Subsection
Real World
\end_layout

\begin_layout Standard
Imagination, hallucination or dreaming are basically autoregressive: Decoupled
 from a finite data sample, the brain ponders what it remembers, hopefully
 arriving at a better model.
 Even more clearly, closing your eyes and giving things a good thought can
 definitely improve your actions.
 
\end_layout

\begin_layout Standard
Another expression of this is [prove]
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{p}(y|x,YX)=\langle\hat{p}(y|x,\hat{Y}\hat{X},YX)\rangle_{\hat{Y},\hat{X}\sim\hat{p}(y|x,YX)}.
\]

\end_inset


\end_layout

\begin_layout Standard
I.e., sampling from your existing model on average does not improve the model.
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Formula 
\begin{align*}
\langle\hat{p}(y|\hat{Y}Y)\rangle_{\hat{Y},\hat{X}\sim\hat{p}(\hat{Y}|YX)} & =\int_{\hat{Y}}\frac{\hat{p}(y,\hat{Y}|Y)\cancel{\hat{p}(\hat{Y}|Y)}}{\cancel{\hat{p}(\hat{Y}|Y)}}\\
 & =\hat{p}(y|Y).
\end{align*}

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
So what is it good for? 
\end_layout

\begin_layout Subsection
Reasoning
\end_layout

\begin_layout Standard
So far, we have adopted a puristic Bayesian inference view.
 However, in practice information needs not only to be present, but 
\emph on
be made accesible
\emph default
.
 This helps to reconcile it with our everyday intuition of pondering indeed
 being helpful for action.
 
\end_layout

\begin_layout Standard
The idea is that sampling from even a premature model will help with exploration
: Maybe you'll get an idea that helps you downstream.
 
\end_layout

\end_body
\end_document
