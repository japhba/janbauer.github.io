#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
Outline
\end_layout

\begin_layout Itemize
define 
\series bold
learning
\series default
: given some input-output training set, generalize to new inputs based on
 inferred priors.
 This is, in its general definition, independent of any parametric substrate!
\end_layout

\begin_layout Itemize
Formal definition: Let 
\begin_inset Formula $f=\mathcal{T}\left[\emptyset,\left(X,Y\right)\right]$
\end_inset

 be a learned function on 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

 from scratch, where 
\begin_inset Formula $\mathcal{T}:\left(X,Y\right)\mapsto f$
\end_inset

 is a 
\begin_inset Quotes eld
\end_inset

training
\begin_inset Quotes erd
\end_inset

 operator that turns data 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

 into functions 
\begin_inset Formula $f$
\end_inset

.
 While it can be defined purely abstractly, it is highly dependent on the
 inductive bias of the model 
\begin_inset Formula $\mathcal{M}$
\end_inset

 .
 Notably, in the ML setting, this includes architecture and the optimization
 procedure, where the latter is influenced by algorithm (ADAM, SGD,...) and
 hyperparameters (learning rate).
\begin_inset Formula 
\[
\mathcal{T}=\mathcal{T}_{\text{model}\,\mathcal{M}}\overset{\text{e.g.}}{=}\mathcal{T}_{\left(\text{CNN},\,\text{ADAM\,@}\text{lr}=.01\,N_{\text{epoch}}=42\right)}.
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
meta-learning
\series default
 is being referred to as learning to learn.
 
\end_layout

\begin_deeper
\begin_layout Itemize
finding good parameters that control the learning process
\end_layout

\begin_layout Itemize
learn a function by optimizing 
\begin_inset Formula $\Omega$
\end_inset

 
\begin_inset Formula 
\[
f_{\Omega}:\:(\text{model}=\text{(architecture},\text{hyperparameters)},\text{task})\mapsto\text{performance}
\]

\end_inset

 as opposed to learning a function by optimizing 
\begin_inset Formula $\theta$
\end_inset

 on 
\begin_inset Formula $(x,y)\in\text{task}$
\end_inset


\begin_inset Formula 
\[
f_{\theta}:x\mapsto y
\]

\end_inset


\end_layout

\begin_layout Itemize
where in either case we are only provided with a finite 
\series bold
training set
\series default
 of examples from which we generalize
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
transfer learning/few-shot learning/continual learning/fine-tuning
\series default
: use the learned repr to 
\series bold
efficiently incorporate new information
\end_layout

\begin_deeper
\begin_layout Itemize
definition.
 A continual learning paradigm 
\begin_inset Formula $\tilde{\mathcal{T}}$
\end_inset

 that results in 
\begin_inset Formula $\tilde{f}=\tilde{\mathcal{T}}[f,X^{+}]$
\end_inset

 is successful if for new 
\begin_inset Quotes eld
\end_inset

fine-tuning
\begin_inset Quotes erd
\end_inset

 training data 
\begin_inset Formula $\left(X^{+},Y^{+}\right)$
\end_inset

, it holds
\begin_inset Formula 
\[
\mathcal{T}\left[f,\left(X^{+},Y^{+}\right)\right]=\tilde{\mathcal{T}}\left[\emptyset,\left(X,X^{+},Y,Y^{+}\right)\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Probabilistic models such as GPs fulfil this by definition.
 It is a notion of consistency.
\end_layout

\begin_layout Itemize
Note: overfitting prevents this, by definition
\end_layout

\begin_layout Itemize
few shot learning does not require a parameter change, it can happen in
 the activity only! -> chatGPT
\end_layout

\begin_deeper
\begin_layout Itemize
give the demonstration
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Generalization
\end_layout

\begin_deeper
\begin_layout Itemize
generalization is about finding the right inductive bias, i.e.
 the question of choosing the right 
\begin_inset Formula $\mathcal{T}$
\end_inset


\end_layout

\begin_layout Itemize
in this sense, learning how to interpret unexpected input by accounting
 for its possibility in the inductive bias enables one to deal well with
 unseen data,
\begin_inset Formula 
\[
\mathcal{T}\left[f,\left(X,Y\right)\right](x^{*})=f^{\star}(x^{*})
\]

\end_inset


\end_layout

\begin_layout Itemize
even on esoteric 
\begin_inset Formula $x^{*}$
\end_inset

.
\end_layout

\begin_layout Itemize
ChatGPT can also do this.
\end_layout

\end_deeper
\end_body
\end_document
