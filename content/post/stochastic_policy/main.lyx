#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Section -> #
\end_layout

\begin_layout Plain Layout
Subsection -> ## and so on
\end_layout

\begin_layout Plain Layout
use array tables instead of align for multiline equations
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Is random behavior helpful in any situation? By definition, random actions
 are the most uninformed, and if any better is known should be suboptimal.
 Yet, the issue is more subtle.
 Reinforcement learning and game theory can be paradigms to reason about
 this.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename featured.png
	lyxscale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Soccer.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Reinforcement learning
\end_layout

\begin_layout Standard
Reinforcement learning is a framework to identify the appropriate actions
 
\begin_inset Formula $\left\{ a_{t}\right\} $
\end_inset

 of an agent 
\begin_inset Formula $\mathcal{A}$
\end_inset

 in an environment 
\begin_inset Formula $\mathcal{E}$
\end_inset

 that will maximize an expected reward 
\begin_inset Formula $\langle\mathcal{R}\left(\left\{ a_{t}\right\} ,\mathcal{E}\left(\left\{ a_{t}\right\} \right)\right)\rangle_{\mathcal{E},t}$
\end_inset

 over time 
\begin_inset Formula $t$
\end_inset

 and possible environments 
\begin_inset Formula $\mathcal{E}$
\end_inset

.
 Note that in general, the actions affect the environment; 
\begin_inset Formula $\mathcal{E}=\mathcal{E}\left(\left\{ a_{t}\right\} \right)$
\end_inset

 and hence the reward.
 
\end_layout

\begin_layout Standard
Consider an endgame in soccer.
 The captain 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is deciding whether to aim to the left or the right of the goal.
 What should the captain do?
\end_layout

\begin_layout Standard
Let's say the goalkeeper has a weak left side and will never go there.
 Then, the captain should always score left and will always hit the goal,
 
\begin_inset Formula $a_{t}^{\star}=L$
\end_inset

.
\end_layout

\begin_layout Standard
Let's say the goalkeeper has a not quite so weak left side and will go there
 40% of the time.
 Should the captain then aim right 40% of the time to cover the cases where
 the goalkeeper might go left? 
\end_layout

\begin_layout Standard
This is a fallacy known as probability matching that human behavior has
 been shown to be susceptible to.
 If we calculate the expected reward however, we see a difference:
\end_layout

\begin_layout Paragraph*
Probability matching
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{array}{ccccccc}
\langle R\rangle & = & 0 & \cdot & p(\text{aim }L) & \cdot & p(\text{jump }L)\\
 & + & 1 & \cdot & p(\text{aim }L) & \cdot & p(\text{jump }R)\\
 & + & 1 & \cdot & p(\text{aim }R) & \cdot & p(\text{aim }L)\\
 & + & 0 & \cdot & p(\text{aim }R) & \cdot & p(\text{aim }R)\\
 & = & 0 & \cdot & .6 & \cdot & .4\\
 & + & 1 & \cdot & .6 & \cdot & .6\\
 & + & 1 & \cdot & .4 & \cdot & .4\\
 & + & 0 & \cdot & .4 & \cdot & .6\\
 & = & 0 & + & .12 & +.12 & +0\\
 & = & 0.52.
\end{array}
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Maximum reward
\end_layout

\begin_layout Standard
Instead, let's always aim for the most likely side, 
\begin_inset Formula $p(\text{aim }\text{L})=1$
\end_inset

:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\begin{array}{ccccccc}
\langle R\rangle & = & 0 & \cdot & 1 & \cdot & .4\\
 & + & 1 & \cdot & 1 & \cdot & .6\\
 & + & 1 & \cdot & 0 & \cdot & .4\\
 & + & 0 & \cdot & 0 & \cdot & .6\\
 & = & 0.6.
\end{array}
\]

\end_inset


\end_layout

\begin_layout Standard
This reveals that probability matching is actually not the right strategy!
\end_layout

\begin_layout Subsection*
Incorporating time
\end_layout

\begin_layout Standard
This result is surprising! Note that we made a critical assumption: There
 is only a single trial, and captain and goalkeeper part and never meet
 again after.
 Let's say we have a clever goalkeeper who learns that when the captain
 shoots left, he will have a preference to do so afterwards (ignoring his
 weak leg).
 In that setting, the 'goalkeeper'-environment 
\begin_inset Formula $\mathcal{E}$
\end_inset

 now changes its response to actions, 
\begin_inset Formula $\mathcal{E}=\mathcal{E}\left(\left\{ a_{t}\right\} \right)$
\end_inset

.
 What is the optimal strategy now?
\end_layout

\begin_layout Standard
The captain now is in bad luck: Whenever he develops a preference, the goalkeepe
r will adapt to it in the very next trial.
 If he held on to this strategy, he would never score a goal again! So should
 he just alternate the sides he targets? The goalkeeper could counteract
 this with likewise just changing sides.
 In fact, any more complicated schedule will eventually be learned by a
 sufficiently clever goalkeeper and hence only give a temporary.
 Even more, throwing a dice every ten shoots will not change this in principle.
 The best we can do here is to either come up with an insanely complicated
 schedule that decides when to shoot left or right (a 
\emph on
pseudorandom
\emph default
 schedule).
 This, as the name suggests, is then indistinguishable in principle from
 a completely random schedule: The captain throws a dice every time and
 minimizes the mutual information of his policy and the action 
\begin_inset Formula $\pi^{\star}=\text{argmin}_{\pi}\mathbb{I}\left[a_{t},a_{t+1}\right],$
\end_inset

 where we now understand 
\begin_inset Formula $\pi=\pi\left(\left\{ a_{t}\right\} \right)$
\end_inset

 as a distribution over sequences of actions 
\begin_inset Formula $a_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
So a stochastic strategy is better than being purely exploitative! (we are
 actually not random, we are using our knowledge about the clever gatekeeper)
\end_layout

\begin_layout Subsection*
A stubborn goalkeeper
\end_layout

\begin_layout Standard
So is soccer then boring in the end? Again, our modelling was too simple:
 What happens when the goalkeeper takes some time to adapt to a change in
 strategy of the captain, let's say on a timescale 
\begin_inset Formula $\tau$
\end_inset

? A rationale for that could be a stubborn goalkeeper: Even though the captain
 now aimed 
\begin_inset Formula $L$
\end_inset

 four times, this surely was just out of random, and I should continue to
 respond at random! (a prior belief about the strategy of the captain).
 The captain might then have an advantage: He on expectation can get 2 more
 goals in after that point, because the goalkeeper will continue to behave
 randomly.
 So, indeed, it can be a game of psychology between the two: When the goalkeeper
 adheres to his beliefs, the captain might be able to leverage an advantage
 and exploit for a couple of trials before adapting.
 
\end_layout

\begin_layout Subsection
Game theory
\end_layout

\begin_layout Standard
Game theory is a mathematical framework to handle adversarial interactions
 as in the sketched soccer scenario.
 It summarizes the outcome of actions into a so called game matrix that
 is shown below.
 Here, the values indicate the reward 
\begin_inset Formula $\mathcal{R}$
\end_inset

 of the captain 
\begin_inset Formula $c$
\end_inset

 over the goalkeeper 
\begin_inset Formula $g$
\end_inset

 for jumping left 
\begin_inset Formula $L$
\end_inset

 or right 
\begin_inset Formula $R$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{array}{ccc}
\mathcal{R} & c_{L} & c_{R}\\
g_{L} & 0 & 1\\
g_{R} & 1 & 0
\end{array}
\]

\end_inset


\end_layout

\begin_layout Standard
Note that if either party where to commit to a strategy (say 
\begin_inset Formula $L$
\end_inset

), the other party could adjust to that by choosing 
\begin_inset Formula $L$
\end_inset

 likewise.
 That way, the captain would maximize 
\begin_inset Formula $\mathcal{R}$
\end_inset

, while the goalkeeper would minimize it.
\end_layout

\begin_layout Standard
The notion of a Nash equilibrium provides a solution to this dilemma: 
\series bold
Both agents should adopt a strategy that will have either party be off worse
 if they depart from it.

\series default
 This is the case for the following probabilities of acting:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{array}{ccc}
\mathcal{R} & c_{L}@.5 & c_{R}@.5\\
g_{L}@.5 & 0 & 1\\
g_{R}@.5 & 1 & 0
\end{array}
\]

\end_inset


\end_layout

\begin_layout Standard
Where does this fit in the language of reinforcment learning above? The
 game-theoretic scenario assumes both the gatekeeper and environment to
 respond optimally, i.e., a stubborn goalkeeper is not considered.
 Then, we let the parties interact and come up with the most complicated
 schedules 
\begin_inset Formula $\left\{ \pi_{t}\right\} $
\end_inset

 and learning schemes 
\begin_inset Formula $\mathcal{E}\left(\left\{ a_{t}\right\} \right)$
\end_inset

 they can imagine.
 Ultimately, 
\begin_inset Formula $\langle\mathcal{R}\left(\left\{ a_{t}\right\} ,\,\mathcal{E}\left(\left\{ a_{t}\right\} \right)\right)\rangle_{a_{t}\sim\pi_{t},\mathcal{E},t}$
\end_inset

 will be bounded from above by 
\begin_inset Formula $.5$
\end_inset

, which is the best average score the captain can achieve (or the smallest
 loss the goalkeeper can try to prevent) in every trial.
\end_layout

\end_body
\end_document
